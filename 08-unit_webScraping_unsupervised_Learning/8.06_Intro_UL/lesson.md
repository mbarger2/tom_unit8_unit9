# Lesson 8.6 Intro to Unsupervised Learning

### Lesson Duration: 3 hours

> Purpose: To introduce Unsupervised Learning and specifically clustering - with its main use cases and its most popular algorithm, K-Means.

---

### Learning Objectives:

After this lesson, students will be able to:

- Explain what UL means in a technical view
- Understand the intuition behind K-Means
- Perform a basic clustering task


### Lesson 1 key concepts

> :clock10: 20 min

- Unsupervised learning

<details>
<summary> Click for description: Unsupervised Learning </summary>

---

Suggestion link for this class: [https://vas3k.com/blog/machine_learning/?ref=hn]
The instructor can share this link with the students in class for reference. 

---


- What is Unsupervised Learning?
  Most of the time, when we discuss Machine Learning, we talk about Supervised Learning: trying to predict a target variable. Linear and logistic regression, Decision Trees, Random Forests and Boosting methods... they all need a target variable to operate. When we don't have a target variable, we are in the domain of Unsupervised Learning.

  If we're not predicting a target variable, what are we doing? We can try to understand the relationships between the features or between the observations in our dataset - discover, perhaps underlying patterns and multivariate relationships that go beyond what humans can spot doing a basic Exploratory Data Analysis.

  It's called Unsupervised learning because we do not have a clear way to tell whether the algorithm is doing well or not - in SL, predictions can be compared with true values (labels) - here there are no performance metrics with which to supervise the task.

- **Clustering**:
  The most popular task in UL is Clustering. It's popular in the business world because companies usually have big unlabeled datasets of customers.They need to segment those customers in order to make effective decisions like: understanding their behavior, sending targeted emails, offering loyalty promotions, tailoring new products, etc. Clustering algorithms find observations with similarities and group themselves.

- **Clustering vs. traditional segmentation**:
  Traditionally, subject experts were the ones creating the segmentation: maybe they had some data showing that, in general, customers under and over 25 years old behave differently, and that gender is a clear divide when it comes to shopping. Rich datasets and clustering algorithms can challenge these traditional segmentation. If an e-commerce site has click-through rates in multiple stages of the marketing funnel, email-opening rates and a history of products searched, added to a wish list and purchased (each divided in a few variables by category of product), it's much more difficult to use "instinct" or a couple of bar charts to segment the customers. However, clustering algorithms can find groups of customers that are similar based on these behavioral variables.

- Other Unsupervised Learning tasks

- _Principal Component Analysis:_ PCA is a method that takes a dataset with _p_ features that are assumed to be somewhat correlated and produces _p_ new features, called Principal Components, ordered in such a way that the first few Principal Components explain most of the variability in the dataset. This particularity means that the Principal Components can reduce the dimensionality of the dataset. This outcome can be used either for exploring the dataset or for improving the performance and training predictive algorithms, at the cost of losing interpretability.

- Other Unsupervised tasks and methods include

  - _Anomaly detection_: using algorithms such as the Local Outlier Factor or Gaussian Mixtures. The name of the task is pretty self explanatory: we're trying to find observations that deviate strongly from the norm. It's useful for fraud detection, detecting defective products in manufacturing or simply for removing / dealing with outliers as part of a broader ML task.

  - _Generative modelling_: using, for example, Generative Adversarial Network (GANs). The goal is to learn patterns of input data in such a way that the model can be used to generate new observations that plausibly could have been drawn from the original dataset It is important to note that this last part of the task is evaluated with SL: we train a model to try to detect whether an observation was part of the original dataset or generated by the model
 
After this overview, we will go back to clustering, since it's what we were asked to do in our project.

</details>

:coffee: **BREAK**

---

#### :pencil2: Check for Understanding - Class activity/quick quiz

> :clock10: 15 min (+ 20 min Review)

<details>
  <summary> Click for Instructions: Activity 1 </summary>

- Link to [8.06 Activity 1](https://github.com/ironhack-edu/data_8.06_activities/blob/master/8.06_activity_1.md).

</details>

<details>
  <summary>Click for Solution: Activity 1 solutions</summary>

- Link to [8.06 Activity 1 solutions](https://gist.github.com/ironhack-edu/357b72e87988067327f6dad055b4cdd4).


</details>

---

:coffee: **BREAK**

---

### Lesson 2 key concepts

> :clock10: 20 min

- Perform a basic clustering task using dummy data

We will use a dataset about wines that's built-in into Scikit-Learn to get started with clustering.
Code along with your students:

**Data loading and preparation:**

```python

import pandas as pd
from sklearn import datasets

data = datasets.load_wine()

X = pd.DataFrame(data["data"], columns=data["feature_names"])
#y = pd.Series(data["target"])

# Comment the different variables of this dataset
# Notice how they are all numerical - KMeans cannot deal with categorical variables
X.head()

# The scale of "proline" is much higher than the scale of many other variables!
# K-Means is a distance based algorithm: we need to scale / normalize:
from sklearn.preprocessing import StandardScaler
X_prep = StandardScaler().fit_transform(X)

# Now, all features will have the same weight.
pd.DataFrame(X_prep).head()
```

**Clustering**

We will pick manually the number of clusters we want - let's set it to 8. Later we will discuss how many clusters should we have.

When randomness is involved (remember, K-means picks randomly the initial centroids), we better use a random seed so that we can reproduce our results. We can set this directly to the argument random_state.

```python

# Clustering:
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=8, random_state=1234)
kmeans.fit(X_prep)

```

We have run the model - now what? We can explore the clusters assigned to each observation:

```python

# Predicting / assigning the clusters:
clusters = kmeans.predict(X_prep)

# Check the size of the clusters
pd.Series(clusters).value_counts().sort_index()

# Explore the cluster assignment in the original dataset
X_df = pd.DataFrame(X)
X_df["cluster"] = clusters
X_df.head()
```

Before going into a deep exploration, we should think about the clustering itself.

Go to the documentation page of K-means and discuss the parameters with your students:
https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html

- **init**: those are the different initialization strategies. By default, Sklearn is already using a _smart_ method, not the totally random one we saw.
- **n_init:** By default, Scikit-Learn has tried 10 different random initializations and kept the best model.

How does Scikit-Learn know which model is the best? It uses a performance metric called **inertia**. It is the mean squared distance between each instance and its closest centroid. It's stored in the `inertia_` attribute of the model.

```python
kmeans.inertia_ # our 'performance metric' should be around 324350
```

- **max_iter:** It iterates up to 300 times by default (those are the re-computing centroids iterations we saw earlier)
- **tol:** Looks like this is a way to determine when to stop iterating (if the clusters have changed only very slightly, we assume we have achieved 'convergence')
- **algorithm:** There are variations in the implementation of most algorithms and K-Means is no exception. By default, we're using a 'smart' implementation called _elkan_.

For learning purposes, we will tweak the parameters to replicate the good ol' K-Means - a bit dumber than the Scikit-Learn, yes, but also the original.

```python
kmeans = KMeans(n_clusters=8,
                init="random",
                n_init=3,  # try with 1, 4, 8, 20, 30, 100...
                max_iter=2,
                tol=0,
                algorithm="full",
                random_state=1234)
kmeans.fit(X_prep)
print(kmeans.inertia_)
```

#### :pencil2: Check for Understanding - Class activity/quick quiz

> :clock10: 20 min (+ 10 min Review)

<details>
  <summary> Click for Instructions: Activity 2 </summary>

- Link to [8.06 Activity 2](https://github.com/ironhack-edu/data_8.06_activities/blob/master/8.06_activity_2.md).

</details>

<details>
  <summary> Activity 2 solutions</summary>

- Link to [8.06 Activity 2 solution](https://gist.github.com/ironhack-edu/f88df6779c7acd1c560eeb28f8836582).

</details>

---

:coffee: **BREAK**

---

### Lesson 3 key concepts

> :clock10: 20 min

- Silhouette Score
- Choosing K

<details>
<summary> Choosing K </summary>

We have used K=8 by default for now - but we know that 8 might not be the optimal number of clusters for our dataset. Having a metric like inertia, we can compute it for several K values and then use the "elbow method" to choose the best K.

We will now leave all other parameters with their default value, since it seems to work pretty well.

```python
import numpy as np
K = range(2, 20)
inertia = []

for k in K:
    kmeans = KMeans(n_clusters=k,
                    random_state=1234)
    kmeans.fit(X_prep)
    inertia.append(kmeans.inertia_)

import matplotlib.pyplot as plt
%matplotlib inline

plt.figure(figsize=(16,8))
plt.plot(K, inertia, 'bx-')
plt.xlabel('k')
plt.ylabel('inertia')
plt.xticks(np.arange(min(K), max(K)+1, 1.0))
plt.title('Elbow Method showing the optimal k')
```

There seems to be an elbow at k=3, and then a very slight one at k=10, but other than that the plot is quite smooth. What if our business needs involv having a k between 5 and 8?

There is another metric that will help us decide.

<details>
<summary> Silhouette Score </summary>

Inertia is the metric that Scikit-Learn optimizes, but it does not have a limited range and that makes it difficult to evaluate.

There's a metric called [Silhouette Score](<https://en.wikipedia.org/wiki/Silhouette_(clustering)>) that also measures how similar is an observation is to its own cluster compared to other clusters. For the *i*th observation, the Silhouette Score is:

`(b - a) / max(a,b)`

Where:

`a` = mean intra-cluster distance (the average distance between _i_ and every other observation in the cluster where _i_ belongs)

`b` = mean nearest-cluster distance (the average distance between _i_ and the observations of the nearest cluster that _i_ is not part of)

The silhouette score for the whole model is the average of all the silhouette scores of each instance.

Because we divide the subtraction of (b-a) by the max of the two distances (which will always be b unless the observation has been wrongly assigned to a cluster it should not belong), we obtain a "normalized score", that ranges from -1 to 1, and that makes it easier to interpret.

```python
from sklearn.metrics import silhouette_score
K = range(2, 20)
silhouette = []

for k in K:
    kmeans = KMeans(n_clusters=k,
                    random_state=1234)
    kmeans.fit(X_prep)
    silhouette.append(silhouette_score(X_prep, kmeans.predict(X_prep)))


plt.figure(figsize=(16,8))
plt.plot(K, silhouette, 'bx-')
plt.xlabel('k')
plt.ylabel('silhouette score')
plt.xticks(np.arange(min(K), max(K)+1, 1.0))
plt.title('Elbow Method showing the optimal k')
```

Here, we confirm that 3 is the best option, but we also notice that 5 a quite good - it meets the 'technical criteria' of having a better score than its predecessor (4), so if it falls within the range of our business demands - it looks like the best candidate.

</details>

---

#### :pencil2: Check for Understanding - Class activity/quick quiz

> :clock10: 10 min (+ 10 min Review)

<details>
  <summary> Click for Instructions: Activity 3 </summary>

<!-- ðŸš¨ðŸš¨ðŸš¨ @guillem: activity missing -->
ðŸš¨ðŸš¨ðŸš¨ missing

</details>

<details>
  <summary>Click for Solution: Activity 3 solutions</summary>

<!-- ðŸš¨ðŸš¨ðŸš¨ @guillem: solution missing -->
ðŸš¨ðŸš¨ðŸš¨ missing

```python

```

</details>

---

:coffee: **BREAK**

---

### Lesson 4 key concepts

> :clock10: 20 min

- Exploring the clusters

<details>
  <summary> Scraped info into a dataframe </summary>

Once you have clusters, it's time to see how they segmented your data.

<!-- ðŸš¨ðŸš¨ðŸš¨ @guillem: lesson missing -->
ðŸš¨ðŸš¨ðŸš¨ missing
```python
TBD
```

</details>

### :pencil2: Practice on key concepts - Lab

> :clock10: 30 min

<details>
  <summary> Click for Instructions: Lab </summary>

- Link to the lab: [https://github.com/ironhack-labs/lab-unsupervised-learning-intro](https://github.com/ironhack-labs/lab-unsupervised-learning-intro)

</details>

<details>
  <summary>Click for Solution: Lab solutions</summary>

<!-- ðŸš¨ðŸš¨ðŸš¨ @guillem: solution missing -->
ðŸš¨ðŸš¨ðŸš¨ missing
```python
TBD
```

</details>

---

:sandwich: **LUNCH BREAK**

---
